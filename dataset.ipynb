{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e85b74a",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a549e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import os\n",
    "import json\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fb5caf",
   "metadata": {},
   "source": [
    "## Single CV Simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb82d6b",
   "metadata": {},
   "source": [
    "For NER fine-tuning BERT model purpose, we need CoNLL format as input training data\n",
    "\n",
    "This section code is how BIO tagging flow from extract pdf into saving the BIO schemas into **CoNLL** format.\n",
    "\n",
    "1. Extract all CV file format into .txt.\n",
    "2. Using [Doccano](!https://github.com/doccano/doccano) library for manual Entity tagging.\n",
    "3. Extract jsonl response and format into CoNLL BIO tagged schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf8c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"cv-example.pdf\")\n",
    "with open(\"output-example.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for page in reader.pages:\n",
    "        f.write(page.extract_text() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80f58334",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"tag-example.jsonl\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        data = json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e6e0cde",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 1,\n",
       " 'text': 'Phone\\nEmail\\n082367903648\\nderrickvericho@gmail.com Project LinkLinkedIn\\nhttps://github.com/DerrickVericho\\nhttps://www.linkedin.com/in/derrick-\\nvericho-268563225/Derrick Vericho\\nUNDERGRADUATE DATA SCIENCE\\nSTUDENT\\nI’m a 5th-semester undergraduate student with a strong passion for data science and artificial intelligence, particularly\\nin building solutions that connect technology with real-world needs. My interest lies in combining data-driven insights\\nwith the financial industry, exploring how AI can create smarter and more impactful services. Through various projects, I\\nhave developed skills in data analysis, machine learning, and system development, while continuously seeking\\nchallenges that push me to grow. ABOUT ME\\nSecond author & Corresponding author on published paper “Vision Transformer and CNNs in Kidney Stone\\nClassification: A Comparative Study” acceptance on ICCSCIACTIVITIES\\nActivist Data Science Club: Research DivisionSKILL\\nRelevant Course: Deep Learning, Machine Learning, Model Deployment, Bayesian Data Analysis, Artificial Intelligence, Data Strucure,\\nAlgorithm Programming, Text Mining, Big Data & Infrastructure, Software Engineering, Database TechnologyEDUCATION\\nData Science, School of Computer Science, GPA: 3.57\\nBinus University 2023 - present\\nProgramming & Tools: Python, SQL, Docker, Git, Node.js, Typescript, n8n\\nML & AI Frameworks: TensorFlow, PyTorch, scikit-learn, FastAPI, LangChain\\nNLP & LLM: HuggingFace, Transformers, Prompt Engineering, RAG, VectorDB\\nDatabases: MySQL, PostgreSQL\\nPROJECTS\\nFinance AI Expert Chatbot\\nDeveloped an AI-powered chatbot that provides personalized insights on stock analysis, crypto trends, and portfolio management.\\nBuilt an interactive web app using Streamlit, integrating conversational AI models for real-time financial Q&A.\\nSimplified complex financial concepts into user-friendly explanations, improving accessibility for students and beginner investors.\\nApplied interdisciplinary skills in AI, finance, and UI/UX design to create a functional and engaging tool.\\nSPY Forecasting using LSTM\\nBuilt a machine learning web app to predict obesity levels based on lifestyle & habit data\\nCompared Random Forest vs. XGBoost models for classification, selected Random Forest due to higher test accuracy (94.31% vs.\\n91.94%) and strong generalization.\\nDesigned intuitive UI/UX to communicate predictions and probabilities integrated with FastAPI as backend\\nObesity Level Prediction Web App\\nBuilt and trained an LSTM-based deep learning model to forecast SPY ETF price movements using historical time series data.\\nDesigned complete pipeline: data collection, preprocessing, sequence generation, model training, and evaluation metrics.\\nAchieved strong predictive performance with: MSE: 0.0013, RMSE: 0.0362, MAE: 0.0261, R ²  Score: 0.8729.\\nCERTIFICATION\\nIBM Data Science\\nDeveloped full data science pipeline (data wrangling, modeling, deployment) using Python, SQL, Jupyter & Watson Studio\\nExecuted real-world projects: house price regression, loan default classification, cross ‑ platform flight reliability dashboard\\nBuilt expertise in ML algorithms (regression, tree ‑ based, clustering), deep learning & generative AI\\nNVIDIA Fundamentals of Deep Learning\\nTrained CNNs for image classification/object detection, leveraging GPU acceleration and model optimization\\nApplied data augmentation & hyperparameter tuning to improve model robustness\\nEmployed transfer learning to accelerate development using pretrained architectures\\nDeployed end-to-end computer vision solutions, including final project (e.g. fresh vs rotten fruit classification)',\n",
       " 'label': [[1192, 1204, 'EDUCATION'],\n",
       "  [1206, 1232, 'EDUCATION'],\n",
       "  [1244, 1260, 'EDUCATION'],\n",
       "  [1297, 1303, 'SKILL'],\n",
       "  [1305, 1308, 'SKILL'],\n",
       "  [1310, 1316, 'TOOLS'],\n",
       "  [1318, 1321, 'TOOLS'],\n",
       "  [1323, 1330, 'SKILL'],\n",
       "  [1332, 1342, 'SKILL'],\n",
       "  [1344, 1347, 'TOOLS'],\n",
       "  [1368, 1378, 'SKILL'],\n",
       "  [1380, 1387, 'SKILL'],\n",
       "  [1389, 1401, 'SKILL'],\n",
       "  [1403, 1410, 'SKILL'],\n",
       "  [1412, 1421, 'SKILL'],\n",
       "  [1433, 1444, 'TOOLS'],\n",
       "  [1446, 1458, 'SKILL'],\n",
       "  [1460, 1478, 'SKILL'],\n",
       "  [1480, 1483, 'SKILL'],\n",
       "  [1485, 1493, 'TOOLS'],\n",
       "  [1505, 1510, 'TOOLS'],\n",
       "  [1512, 1522, 'TOOLS']],\n",
       " 'Comments': []}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac6c2ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def jsonl_to_conll(jsonl_file, output_file):\n",
    "    \"\"\"Conver JSONL into ConLL format\"\"\"\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
    "        for entry in data:\n",
    "            text = entry[\"text\"] # Get the text from doc\n",
    "            labels = entry.get(\"label\", []) # Get the labels from doc\n",
    "            \n",
    "            doc = nlp(text) # Exract the doc into token\n",
    "            tags = [\"O\"] * len(doc) # Initialize tags\n",
    "\n",
    "            # Get position and label\n",
    "            for start, end, label in labels:\n",
    "                # Get each token id and token\n",
    "                for token_id, token in enumerate(doc):\n",
    "                    # Set Begin and Inside Label if the label is available\n",
    "                    if token.idx >= start and token.idx < end:\n",
    "                        prefix = \"B-\" if token.idx == start else \"I-\"\n",
    "                        tags[token_id] = prefix + label\n",
    "\n",
    "            for token, tag in zip(doc, tags):\n",
    "                out.write(f\"{token.text} {tag}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "jsonl_to_conll(\"tag-example.jsonl\", \"tag-example.conll\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3af721f",
   "metadata": {},
   "source": [
    "## CV PDF Tagging Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e7f09d",
   "metadata": {},
   "source": [
    "### Extract PDF into txt format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44c35904",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_folder = \"CV_pdf\"\n",
    "output_folder = \"CV_txt\"\n",
    "\n",
    "# Loop semua file di folder\n",
    "for file_name in os.listdir(input_folder):\n",
    "    if file_name.endswith(\".pdf\"):\n",
    "        pdf_path = os.path.join(input_folder, file_name)\n",
    "        txt_path = os.path.join(output_folder, file_name.replace(\".pdf\", \".txt\"))\n",
    "\n",
    "        # Baca PDF\n",
    "        reader = PdfReader(pdf_path)\n",
    "        with open(txt_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for page in reader.pages:\n",
    "                f.write(page.extract_text() + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54183587",
   "metadata": {},
   "source": [
    "### Extract tagging jsonl BIO format into CoNLL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee4583cc",
   "metadata": {},
   "source": [
    "There are some doccano issue, where the output format may labelled with:\n",
    "- Array of arrays -> [start_offset, end_offset, \"LABEL\"]\n",
    "- Self descriptive format (with dictionary) -> {\"entity\": [{id, \"LABEL\", start, end}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7908636b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set current id to name the output file\n",
    "current_id = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac9058d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for array of arrays\n",
    "# Set the id with your prefered start number\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def jsonl_to_conll(jsonl_file, output_dir, current_id=current_id):\n",
    "    \"\"\"Conver JSONL into ConLL format\"\"\"\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    for entry in data:\n",
    "        text = entry[\"text\"] # Get the text from doc\n",
    "        labels = entry.get(\"label\", []) # Get the labels from doc\n",
    "        \n",
    "        doc = nlp(text) # Exract the doc into token\n",
    "        tags = [\"O\"] * len(doc) # Initialize tags\n",
    "\n",
    "        # Get position and label\n",
    "        for start, end, label in labels:\n",
    "            # Get each token id and token\n",
    "            for token_id, token in enumerate(doc):\n",
    "                # Set Begin and Inside Label if the label is available\n",
    "                if token.idx >= start and token.idx < end:\n",
    "                    prefix = \"B-\" if token.idx == start else \"I-\"`\n",
    "                    tags[token_id] = prefix + label\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"CV_{current_id}.conll\")\n",
    "        current_id+=1\n",
    "        \n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as out:\n",
    "            for token, tag in zip(doc, tags):\n",
    "                out.write(f\"{token.text} {tag}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "jsonl_to_conll(\"tagged_jsonl/all.jsonl\", \"tagged_conll\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d7f47609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this for self decriptive format\n",
    "# Set the id with your prefered start number\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def jsonl_to_conll(jsonl_file, output_dir, current_id=current_id):\n",
    "    \"\"\"Conver JSONL into ConLL format\"\"\"\n",
    "    with open(jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = [json.loads(line) for line in f]\n",
    "\n",
    "    for entry in data:\n",
    "        text = entry[\"text\"] # Get the text from doc\n",
    "        entities = entry.get(\"entities\", []) # Get the labels from doc\n",
    "        \n",
    "        doc = nlp(text) # Exract the doc into token\n",
    "        tags = [\"O\"] * len(doc) # Initialize tags\n",
    "\n",
    "        # Get position and label\n",
    "        for ent in entities:\n",
    "            start, end, label = ent[\"start_offset\"], ent[\"end_offset\"], ent[\"label\"]\n",
    "            for token_id, token in enumerate(doc):\n",
    "                if token.idx >= start and token.idx < end:\n",
    "                    prefix = \"B-\" if token.idx == start else \"I-\"\n",
    "                    tags[token_id] = prefix + label\n",
    "\n",
    "        output_file = os.path.join(output_dir, f\"CV_{current_id}.conll\")\n",
    "        current_id+=1\n",
    "        \n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as out:\n",
    "            for token, tag in zip(doc, tags):\n",
    "                out.write(f\"{token.text} {tag}\\n\")\n",
    "            out.write(\"\\n\")\n",
    "\n",
    "jsonl_to_conll(\"tagged_jsonl/all.jsonl\", \"tagged_conll\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llms",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
